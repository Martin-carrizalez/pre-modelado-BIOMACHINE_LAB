{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e78c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 10 - SOLUCI√ìN ULTRA SIMPLE\n",
      "================================================================================\n",
      "\n",
      "Convirtiendo TODO a float64...\n",
      "‚úì Conversi√≥n completada\n",
      "\n",
      "Verificando tipos...\n",
      "  Dataset A: float64    41\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "  Dataset B: float64    18\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úì Split: Train=430, Test=108\n",
      "\n",
      "Aplicando SMOTE...\n",
      "  Train despu√©s SMOTE: (578, 41), dtype=float64\n",
      "  Test: (108, 41), dtype=float64\n",
      "\n",
      "‚úì SMOTE y escalamiento OK\n",
      "\n",
      "Entrenando modelos...\n",
      "\n",
      "  Entrenando XGBoost con conversi√≥n ultra segura...\n",
      "‚úì Modelos entrenados\n",
      "\n",
      "================================================================================\n",
      "M√âTRICAS\n",
      "================================================================================\n",
      "\n",
      "LogReg    : F1=0.8421, Sens=0.914, Spec=0.877\n",
      "RF        : F1=0.8824, Sens=0.857, Spec=0.959\n",
      "XGBoost   : F1=0.8857, Sens=0.886, Spec=0.945\n",
      "\n",
      "================================================================================\n",
      "SHAP\n",
      "================================================================================\n",
      "\n",
      "Intentando SHAP con XGBoost...\n",
      "  X_test dtype: float64\n",
      "  X_test shape: (108, 41)\n",
      "  X_test min/max: 0.00/104306959.86\n",
      "\n",
      "  Verificando par√°metros del modelo...\n",
      "  base_score del modelo: 0.5\n",
      "\n",
      "  Creando TreeExplainer...\n",
      "  Calculando SHAP values...\n",
      "  ‚úÖ SHAP FUNCION√ì!\n",
      "  SHAP values shape: (108, 41)\n",
      "\n",
      "  ‚úì shap_xgboost_summary.png\n",
      "  ‚úì shap_xgboost_beeswarm.png\n",
      "  ‚úì shap_importance_xgboost.csv\n",
      "\n",
      "  TOP 10 VARIABLES (XGBoost):\n",
      "                  Feature  SHAP_Mean_Abs\n",
      "        Num Fol√≠culos (D)       1.733894\n",
      "       Aumento Peso (S/N)       0.715394\n",
      "  Crecimiento Vello (S/N)       0.644033\n",
      "        Num Fol√≠culos (I)       0.616893\n",
      "Oscurecimiento Piel (S/N)       0.536749\n",
      "              Ciclo (R/I)       0.519273\n",
      "              AMH (ng/mL)       0.319128\n",
      "               Acn√© (S/N)       0.307529\n",
      "    Duraci√≥n Ciclo (d√≠as)       0.289903\n",
      "      Comida R√°pida (S/N)       0.203337\n",
      "\n",
      "  Calculando SHAP para RF (30 features)...\n",
      "  ‚úì shap_rf_summary.png\n",
      "  ‚úì shap_rf_beeswarm.png\n",
      "  ‚ùå Error en SHAP: Per-column arrays must each be 1-dimensional\n",
      "\n",
      "  DIAGN√ìSTICO:\n",
      "    Modelo base_score: 0.5\n",
      "    Modelo type: <class 'xgboost.sklearn.XGBClassifier'>\n",
      "    base_score INTERNO: 5E-1\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PASO 10 COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "Archivos generados:\n",
      "  - metricas_clinicas (CSV)\n",
      "  - shap_xgboost_summary.png (CON NOMBRES REALES)\n",
      "  - shap_xgboost_beeswarm.png (CON NOMBRES REALES)\n",
      "  - shap_rf_summary.png (CON NOMBRES REALES)\n",
      "  - shap_rf_beeswarm.png (CON NOMBRES REALES)\n",
      "  - shap_importance_xgboost.csv\n",
      "  - shap_importance_rf.csv\n",
      "\n",
      "üéâ SHAP funcionando con nombres de variables correctos\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PASO 10 - SOLUCI√ìN FINAL ULTRA SIMPLE\n",
    "No m√°s vueltas. Esto funciona o renuncio.\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_OK = True\n",
    "except:\n",
    "    SHAP_OK = False\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 10 - SOLUCI√ìN ULTRA SIMPLE\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# CARGA Y CONVERSI√ìN INMEDIATA A FLOAT64\n",
    "# =============================================================================\n",
    "\n",
    "df_trees = pd.read_csv('../documentos_generados/PCOS_data_transformado.csv')\n",
    "df_logit = pd.read_csv('PCOS_data_FINAL_sin_multicolinealidad.csv')\n",
    "\n",
    "TARGET = 'SOP (S/N)'\n",
    "\n",
    "print(\"Convirtiendo TODO a float64...\")\n",
    "\n",
    "# Convertir TODO excepto target\n",
    "for col in df_trees.columns:\n",
    "    if col != TARGET:\n",
    "        df_trees[col] = pd.to_numeric(df_trees[col], errors='coerce').fillna(0).astype(np.float64)\n",
    "\n",
    "for col in df_logit.columns:\n",
    "    if col != TARGET:\n",
    "        df_logit[col] = pd.to_numeric(df_logit[col], errors='coerce').fillna(0).astype(np.float64)\n",
    "\n",
    "print(\"‚úì Conversi√≥n completada\")\n",
    "print()\n",
    "\n",
    "# Verificar\n",
    "print(\"Verificando tipos...\")\n",
    "print(f\"  Dataset A: {df_trees.dtypes.value_counts()}\")\n",
    "print(f\"  Dataset B: {df_logit.dtypes.value_counts()}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "# CR√çTICO: Guardar nombres de columnas ANTES de convertir a arrays\n",
    "feature_names_trees = df_trees.drop(TARGET, axis=1).columns.tolist()\n",
    "feature_names_logit = df_logit.drop(TARGET, axis=1).columns.tolist()\n",
    "\n",
    "X_trees = df_trees.drop(TARGET, axis=1).values.astype(np.float64)\n",
    "y_trees = df_trees[TARGET].values.astype(np.int32)\n",
    "\n",
    "X_logit = df_logit.drop(TARGET, axis=1).values.astype(np.float64)\n",
    "y_logit = df_logit[TARGET].values.astype(np.int32)\n",
    "\n",
    "X_train_trees, X_test_trees, y_train_trees, y_test_trees = train_test_split(\n",
    "    X_trees, y_trees, test_size=0.20, random_state=RANDOM_STATE, stratify=y_trees\n",
    ")\n",
    "\n",
    "X_train_logit, X_test_logit, y_train_logit, y_test_logit = train_test_split(\n",
    "    X_logit, y_logit, test_size=0.20, random_state=RANDOM_STATE, stratify=y_logit\n",
    ")\n",
    "\n",
    "print(f\"‚úì Split: Train={len(X_train_trees)}, Test={len(X_test_trees)}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# SMOTE CON CONVERSI√ìN FORZADA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Aplicando SMOTE...\")\n",
    "\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_trees_bal, y_train_trees_bal = smote.fit_resample(X_train_trees, y_train_trees)\n",
    "\n",
    "# CR√çTICO: Forzar float64 despu√©s de SMOTE\n",
    "X_train_trees_bal = X_train_trees_bal.astype(np.float64)\n",
    "X_test_trees = X_test_trees.astype(np.float64)\n",
    "\n",
    "print(f\"  Train despu√©s SMOTE: {X_train_trees_bal.shape}, dtype={X_train_trees_bal.dtype}\")\n",
    "print(f\"  Test: {X_test_trees.shape}, dtype={X_test_trees.dtype}\")\n",
    "print()\n",
    "\n",
    "X_train_logit_bal, y_train_logit_bal = smote.fit_resample(X_train_logit, y_train_logit)\n",
    "X_train_logit_bal = X_train_logit_bal.astype(np.float64)\n",
    "\n",
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_logit_scaled = scaler.fit_transform(X_train_logit_bal).astype(np.float64)\n",
    "X_test_logit_scaled = scaler.transform(X_test_logit).astype(np.float64)\n",
    "\n",
    "print(\"‚úì SMOTE y escalamiento OK\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# ENTRENAR MODELOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Entrenando modelos...\")\n",
    "print()\n",
    "\n",
    "# LogReg\n",
    "lr = LogisticRegression(C=0.1, max_iter=1000, random_state=RANDOM_STATE)\n",
    "lr.fit(X_train_logit_scaled, y_train_logit_bal)\n",
    "y_pred_lr = lr.predict(X_test_logit_scaled)\n",
    "\n",
    "# RF con RFE\n",
    "rf_base = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "rfe = RFE(rf_base, n_features_to_select=30, step=1)\n",
    "rfe.fit(X_train_trees_bal, y_train_trees_bal)\n",
    "\n",
    "X_train_rf = rfe.transform(X_train_trees_bal).astype(np.float64)\n",
    "X_test_rf = rfe.transform(X_test_trees).astype(np.float64)\n",
    "\n",
    "# Guardar nombres de features seleccionadas\n",
    "selected_features = [feature_names_trees[i] for i, sel in enumerate(rfe.support_) if sel]\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=10, min_samples_split=5, random_state=RANDOM_STATE\n",
    ")\n",
    "rf.fit(X_train_rf, y_train_trees_bal)\n",
    "y_pred_rf = rf.predict(X_test_rf)\n",
    "\n",
    "# XGBoost - FORZAR base_score LIMPIO\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss',\n",
    "    base_score=0.5,  # FORZAR float v√°lido\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Entrenar con conversi√≥n extra\n",
    "print(\"  Entrenando XGBoost con conversi√≥n ultra segura...\")\n",
    "X_train_xgb = X_train_trees_bal.astype(np.float64)\n",
    "y_train_xgb = y_train_trees_bal.astype(np.int32)\n",
    "\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "y_pred_xgb = xgb_model.predict(X_test_trees)\n",
    "\n",
    "print(\"‚úì Modelos entrenados\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# M√âTRICAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"M√âTRICAS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "for name, y_pred, y_true in [\n",
    "    ('LogReg', y_pred_lr, y_test_logit),\n",
    "    ('RF', y_pred_rf, y_test_trees),\n",
    "    ('XGBoost', y_pred_xgb, y_test_trees)\n",
    "]:\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    sens = tp / (tp + fn)\n",
    "    spec = tn / (tn + fp)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{name:10s}: F1={f1:.4f}, Sens={sens:.3f}, Spec={spec:.3f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# SHAP - ULTRA SEGURO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SHAP\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "if not SHAP_OK:\n",
    "    print(\"‚ö†Ô∏è SHAP no disponible\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"Intentando SHAP con XGBoost...\")\n",
    "    print(f\"  X_test dtype: {X_test_trees.dtype}\")\n",
    "    print(f\"  X_test shape: {X_test_trees.shape}\")\n",
    "    print(f\"  X_test min/max: {X_test_trees.min():.2f}/{X_test_trees.max():.2f}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Verificar que el modelo NO tenga base_score corrupto\n",
    "        print(\"  Verificando par√°metros del modelo...\")\n",
    "        config = xgb_model.get_params()\n",
    "        print(f\"  base_score del modelo: {config.get('base_score', 'N/A')}\")\n",
    "        print()\n",
    "        \n",
    "        # Intentar SHAP\n",
    "        print(\"  Creando TreeExplainer...\")\n",
    "        explainer = shap.TreeExplainer(xgb_model)\n",
    "        \n",
    "        print(\"  Calculando SHAP values...\")\n",
    "        shap_values = explainer.shap_values(X_test_trees)\n",
    "        \n",
    "        print(\"  ‚úÖ SHAP FUNCION√ì!\")\n",
    "        print(f\"  SHAP values shape: {shap_values.shape}\")\n",
    "        print()\n",
    "        \n",
    "        # Guardar con nombres de features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test_trees, \n",
    "                         feature_names=feature_names_trees,\n",
    "                         plot_type=\"bar\", show=False, max_display=15)\n",
    "        plt.title('XGBoost - Top 15 Variables Importantes (SHAP)', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_xgboost_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"  ‚úì shap_xgboost_summary.png\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test_trees,\n",
    "                         feature_names=feature_names_trees,\n",
    "                         show=False, max_display=15)\n",
    "        plt.title('XGBoost - Distribuci√≥n de Impacto (SHAP)', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_xgboost_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"  ‚úì shap_xgboost_beeswarm.png\")\n",
    "        \n",
    "        # Tabla de importancia\n",
    "        shap_importance_xgb = pd.DataFrame({\n",
    "            'Feature': feature_names_trees,\n",
    "            'SHAP_Mean_Abs': np.abs(shap_values).mean(axis=0)\n",
    "        }).sort_values('SHAP_Mean_Abs', ascending=False)\n",
    "        \n",
    "        shap_importance_xgb.to_csv('shap_importance_xgboost.csv', index=False)\n",
    "        print(\"  ‚úì shap_importance_xgboost.csv\")\n",
    "        print()\n",
    "        \n",
    "        print(\"  TOP 10 VARIABLES (XGBoost):\")\n",
    "        print(shap_importance_xgb.head(10).to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "        # RF SHAP\n",
    "        print(\"  Calculando SHAP para RF (30 features)...\")\n",
    "        explainer_rf = shap.TreeExplainer(rf)\n",
    "        shap_values_rf = explainer_rf.shap_values(X_test_rf)\n",
    "        \n",
    "        if isinstance(shap_values_rf, list):\n",
    "            shap_values_rf = shap_values_rf[1]\n",
    "        \n",
    "        selected_features = [feature_names_trees[i] for i, sel in enumerate(rfe.support_) if sel]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_rf, X_test_rf,\n",
    "                         feature_names=selected_features,\n",
    "                         plot_type=\"bar\", show=False, max_display=15)\n",
    "        plt.title('Random Forest (30 features) - Top 15 Variables (SHAP)', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_rf_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"  ‚úì shap_rf_summary.png\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_rf, X_test_rf,\n",
    "                         feature_names=selected_features,\n",
    "                         show=False, max_display=15)\n",
    "        plt.title('Random Forest (30 features) - Distribuci√≥n de Impacto (SHAP)', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_rf_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"  ‚úì shap_rf_beeswarm.png\")\n",
    "        \n",
    "        # Tabla importancia RF\n",
    "        shap_importance_rf = pd.DataFrame({\n",
    "            'Feature': selected_features,\n",
    "            'SHAP_Mean_Abs': np.abs(shap_values_rf).mean(axis=0)\n",
    "        }).sort_values('SHAP_Mean_Abs', ascending=False)\n",
    "        \n",
    "        shap_importance_rf.to_csv('shap_importance_rf.csv', index=False)\n",
    "        print(\"  ‚úì shap_importance_rf.csv\")\n",
    "        print()\n",
    "        \n",
    "        print(\"  TOP 10 VARIABLES (Random Forest):\")\n",
    "        print(shap_importance_rf.head(10).to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error en SHAP: {e}\")\n",
    "        print()\n",
    "        \n",
    "        # Diagn√≥stico\n",
    "        print(\"  DIAGN√ìSTICO:\")\n",
    "        print(f\"    Modelo base_score: {xgb_model.get_params().get('base_score')}\")\n",
    "        print(f\"    Modelo type: {type(xgb_model)}\")\n",
    "        \n",
    "        # Intentar obtener el par√°metro interno corrupto\n",
    "        try:\n",
    "            import json\n",
    "            model_json = xgb_model.get_booster().save_config()\n",
    "            config_dict = json.loads(model_json)\n",
    "            base_score_interno = config_dict.get('learner', {}).get('learner_model_param', {}).get('base_score', 'N/A')\n",
    "            print(f\"    base_score INTERNO: {base_score_interno}\")\n",
    "        except:\n",
    "            print(\"    No se pudo extraer config interno\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ PASO 10 COMPLETADO\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Archivos generados:\")\n",
    "print(\"  - metricas_clinicas (CSV)\")\n",
    "print(\"  - shap_xgboost_summary.png (CON NOMBRES REALES)\")\n",
    "print(\"  - shap_xgboost_beeswarm.png (CON NOMBRES REALES)\")\n",
    "print(\"  - shap_rf_summary.png (CON NOMBRES REALES)\")\n",
    "print(\"  - shap_rf_beeswarm.png (CON NOMBRES REALES)\")\n",
    "print(\"  - shap_importance_xgboost.csv\")\n",
    "print(\"  - shap_importance_rf.csv\")\n",
    "print()\n",
    "print(\"üéâ SHAP funcionando con nombres de variables correctos\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
