{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c333c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PASO 10: MEJORA Y VALIDACIÃ“N AVANZADA DE MODELOS\n",
      "================================================================================\n",
      "\n",
      "âœ“ SHAP disponible\n",
      "âœ“ Imports completados\n",
      "\n",
      "================================================================================\n",
      "SECCIÃ“N 1: CARGA DE DATOS Y PREPARACIÃ“N\n",
      "================================================================================\n",
      "\n",
      "Cargando datasets...\n",
      "Limpiando formato de datos...\n",
      "Verificando tipos de datos...\n",
      "âœ“ Dataset A limpio: (538, 42)\n",
      "âœ“ Dataset B limpio: (538, 19)\n",
      "\n",
      "Preparando datos...\n",
      "âœ“ Datos preparados (split, SMOTE, escalamiento)\n",
      "\n",
      "================================================================================\n",
      "FASE 1: MÃ‰TRICAS CLÃNICAS EXTENDIDAS\n",
      "================================================================================\n",
      "\n",
      "Entrenando modelos para evaluaciÃ³n...\n",
      "\n",
      "âœ“ Modelos entrenados\n",
      "\n",
      "MÃ‰TRICAS CLÃNICAS EXTENDIDAS:\n",
      "\n",
      "              Model  TN  FP  FN  TP  Accuracy  Precision (PPV)  Recall (Sensibilidad)  Especificidad  F1-Score      NPV      LR+      LR-\n",
      "Logistic Regression  63  10   4  31  0.870370         0.756098               0.885714       0.863014  0.815789 0.940299 6.465714 0.132426\n",
      "      Random Forest  66   7   6  29  0.879630         0.805556               0.828571       0.904110  0.816901 0.916667 8.640816 0.189610\n",
      "            XGBoost  66   7   6  29  0.879630         0.805556               0.828571       0.904110  0.816901 0.916667 8.640816 0.189610\n",
      "                KNN  58  15   5  30  0.814815         0.666667               0.857143       0.794521  0.750000 0.920635 4.171429 0.179803\n",
      "\n",
      "âœ“ Guardado: metricas_clinicas_extendidas.csv\n",
      "\n",
      "INTERPRETACIÃ“N CLÃNICA:\n",
      "\n",
      "Sensibilidad (Recall): % de SOP reales detectados\n",
      "  â†’ Alto = Menos falsos negativos (no miss casos SOP)\n",
      "\n",
      "Especificidad: % de No-SOP correctamente identificados\n",
      "  â†’ Alto = Menos falsos positivos (menos alarmas falsas)\n",
      "\n",
      "PPV (Precision): Si predice SOP, Â¿cuÃ¡l es probabilidad real?\n",
      "  â†’ Alto = PredicciÃ³n SOP confiable\n",
      "\n",
      "NPV: Si predice No-SOP, Â¿cuÃ¡l es probabilidad real?\n",
      "  â†’ Alto = PredicciÃ³n negativa confiable\n",
      "\n",
      "================================================================================\n",
      "FASE 2: EXPLICABILIDAD CON SHAP VALUES\n",
      "================================================================================\n",
      "\n",
      "Calculando SHAP values para XGBoost...\n",
      "(Esto puede tardar 2-3 minutos)\n",
      "\n",
      "âš ï¸ Error en SHAP: could not convert string to float: '[5E-1]'\n",
      "   Continuando sin anÃ¡lisis SHAP...\n",
      "\n",
      "âœ“ FASE 2 completada\n",
      "\n",
      "================================================================================\n",
      "FASE 3: ENSEMBLE - VOTING CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "Creando ensemble RF + XGBoost...\n",
      "\n",
      "RESULTADOS ENSEMBLE:\n",
      "  Accuracy:  0.8796\n",
      "  Precision: 0.8235\n",
      "  Recall:    0.8000\n",
      "  F1-Score:  0.8116\n",
      "  ROC-AUC:   0.9417\n",
      "\n",
      "COMPARACIÃ“N:\n",
      "\n",
      "            Model  Accuracy  Precision   Recall  F1-Score  ROC-AUC\n",
      "    Random Forest   0.87963   0.805556 0.828571  0.816901 0.939726\n",
      "          XGBoost   0.87963   0.805556 0.828571  0.816901 0.949119\n",
      "Ensemble (RF+XGB)   0.87963   0.823529 0.800000  0.811594 0.941683\n",
      "\n",
      "âœ“ Guardado: ensemble_comparison.csv\n",
      "\n",
      "MÃ‰TRICAS CLÃNICAS ENSEMBLE:\n",
      "            Model  TN  FP  FN  TP  Accuracy  Precision (PPV)  Recall (Sensibilidad)  Especificidad  F1-Score      NPV      LR+     LR-\n",
      "Ensemble (RF+XGB)  67   6   7  28   0.87963         0.823529                    0.8       0.917808  0.811594 0.905405 9.733333 0.21791\n",
      "\n",
      "================================================================================\n",
      "FASE 4: OPTIMIZACIÃ“N RFE - REDUCIR A 20 FEATURES\n",
      "================================================================================\n",
      "\n",
      "Probando RFE con diferentes nÃºmeros de features...\n",
      "\n",
      "  10 features: CV F1=0.9303Â±0.0277, Test F1=0.8108\n",
      "  15 features: CV F1=0.9250Â±0.0402, Test F1=0.7945\n",
      "  20 features: CV F1=0.9256Â±0.0416, Test F1=0.8333\n",
      "  25 features: CV F1=0.9327Â±0.0210, Test F1=0.8451\n",
      "  30 features: CV F1=0.9240Â±0.0354, Test F1=0.8732\n",
      "\n",
      "âœ“ Guardado: rfe_optimization_results.csv\n",
      "\n",
      "âœ“ Mejor nÃºmero de features: 30\n",
      "\n",
      "âœ“ Guardado: rfe_optimization_curve.png\n",
      "\n",
      "================================================================================\n",
      "FASE 5: VALIDACIÃ“N REPETIDA (REPEATED STRATIFIED K-FOLD)\n",
      "================================================================================\n",
      "\n",
      "ConfiguraciÃ³n: 5-Fold Ã— 3 repeticiones = 15 evaluaciones\n",
      "(Esto puede tardar 5-10 minutos)\n",
      "\n",
      "Validando Logistic Regression...\n",
      "  âœ“ F1 = 0.9108 Â± 0.0210\n",
      "Validando Random Forest (40f)...\n",
      "  âœ“ F1 = 0.9248 Â± 0.0214\n",
      "Validando XGBoost...\n",
      "  âœ“ F1 = 0.9211 Â± 0.0198\n",
      "Validando Ensemble (RF+XGB)...\n",
      "  âœ“ F1 = 0.9228 Â± 0.0191\n",
      "\n",
      "RESULTADOS VALIDACIÃ“N REPETIDA:\n",
      "\n",
      "              Model  Accuracy_mean  Accuracy_std  Precision_mean  Precision_std  Recall_mean  Recall_std  F1_mean   F1_std  ROC_AUC_mean  ROC_AUC_std\n",
      "Logistic Regression       0.910045      0.020610        0.903819       0.030110     0.919318    0.036549 0.910769 0.020987      0.964326     0.011281\n",
      "Random Forest (40f)       0.923893      0.021584        0.914171       0.028147     0.936600    0.029976 0.924838 0.021412      0.977087     0.013090\n",
      "            XGBoost       0.919290      0.020949        0.902677       0.030155     0.941178    0.025028 0.921110 0.019845      0.974123     0.012012\n",
      "  Ensemble (RF+XGB)       0.921594      0.019187        0.908071       0.023976     0.938879    0.029427 0.922841 0.019139      0.976634     0.012957\n",
      "\n",
      "âœ“ Guardado: validacion_repetida_results.csv\n",
      "\n",
      "âœ“ Guardado: validacion_repetida_intervals.png\n",
      "\n",
      "================================================================================\n",
      "REPORTE FINAL Y RECOMENDACIONES\n",
      "================================================================================\n",
      "\n",
      "ðŸ† MODELO RECOMENDADO:\n",
      "   Random Forest (40f)\n",
      "   F1-Score: 0.9248 Â± 0.0214\n",
      "\n",
      "âœ… MODELO INDIVIDUAL Ã“PTIMO: Random Forest (40f)\n",
      "   Ensemble no aporta mejora significativa\n",
      "\n",
      "ðŸ“Š REDUCCIÃ“N DE FEATURES EXITOSA:\n",
      "   30 features mantienen performance\n",
      "   Simplificar modelo sin perder accuracy\n",
      "\n",
      "ðŸ“‹ RECOMENDACIONES PARA TESIS/PAPER:\n",
      "\n",
      "1. MODELO FINAL:\n",
      "   â†’ Usar Random Forest (40f)\n",
      "   â†’ F1 = 0.9248 (validaciÃ³n cruzada repetida)\n",
      "\n",
      "2. MÃ‰TRICAS CLÃNICAS A REPORTAR:\n",
      "   â†’ Sensibilidad: 82.9%\n",
      "   â†’ Especificidad: 90.4%\n",
      "   â†’ PPV: 80.6%\n",
      "   â†’ NPV: 91.7%\n",
      "\n",
      "3. EXPLICABILIDAD:\n",
      "   â†’ Instalar SHAP para anÃ¡lisis de explicabilidad\n",
      "\n",
      "4. VALIDACIÃ“N:\n",
      "   â†’ Mencionar validaciÃ³n repetida 5Ã—3\n",
      "   â†’ Reportar intervalos de confianza\n",
      "   â†’ Robustez demostrada\n",
      "\n",
      "âœ“ Reporte guardado: reporte_final_paso10.json\n",
      "\n",
      "================================================================================\n",
      "ARCHIVOS GENERADOS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ MÃ‰TRICAS Y RESULTADOS:\n",
      "  1. metricas_clinicas_extendidas.csv\n",
      "  2. ensemble_comparison.csv\n",
      "  3. rfe_optimization_results.csv\n",
      "  4. validacion_repetida_results.csv\n",
      "\n",
      "ðŸ“Š VISUALIZACIONES:\n",
      "  9. rfe_optimization_curve.png\n",
      "  10. validacion_repetida_intervals.png\n",
      "\n",
      "ðŸ“„ REPORTES:\n",
      "  11. reporte_final_paso10.json\n",
      "\n",
      "================================================================================\n",
      "âœ… PASO 10 COMPLETADO EXITOSAMENTE\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ‰ ANÃLISIS COMPLETO\n",
      "\n",
      "Has completado:\n",
      "  âœ“ MÃ©tricas clÃ­nicas extendidas\n",
      "  âœ“ Explicabilidad (SHAP)\n",
      "  âœ“ Ensemble optimization\n",
      "  âœ“ RFE refinement\n",
      "  âœ“ ValidaciÃ³n cruzada repetida\n",
      "\n",
      "Tu proyecto estÃ¡ listo para:\n",
      "  â†’ Defensa de tesis\n",
      "  â†’ PublicaciÃ³n cientÃ­fica\n",
      "  â†’ PresentaciÃ³n a comitÃ© biomÃ©dico\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PASO 10: MEJORA Y VALIDACIÃ“N AVANZADA DE MODELOS\n",
    "================================================================================\n",
    "\n",
    "Proyecto: PredicciÃ³n de SÃ­ndrome de Ovario PoliquÃ­stico (SOP)\n",
    "InstituciÃ³n: ClÃºster de IngenierÃ­a BiomÃ©dica del Estado de Jalisco\n",
    "Fecha: 2 noviembre 2025\n",
    "Autor: Usuario (QFB) + Claude (Asistente)\n",
    "\n",
    "OBJETIVO:\n",
    "Refinar y validar exhaustivamente los modelos del Paso 9\n",
    "\n",
    "CONTENIDO:\n",
    "FASE 1 - MÃ‰TRICAS CLÃNICAS:\n",
    "  - Sensibilidad, Especificidad, PPV, NPV\n",
    "  - Matrices de confusiÃ³n extendidas\n",
    "  \n",
    "FASE 2 - EXPLICABILIDAD:\n",
    "  - SHAP values (XGBoost y Random Forest)\n",
    "  - InterpretaciÃ³n top 10 features\n",
    "  \n",
    "FASE 3 - ENSEMBLE:\n",
    "  - Voting Classifier (RF + XGBoost)\n",
    "  - ComparaciÃ³n vs modelos individuales\n",
    "  \n",
    "FASE 4 - OPTIMIZACIÃ“N RFE:\n",
    "  - Reducir features de 40 a 20\n",
    "  - Mantener performance\n",
    "  \n",
    "FASE 5 - VALIDACIÃ“N REPETIDA:\n",
    "  - Repeated Stratified K-Fold (5Ã—3)\n",
    "  - Intervalos de confianza\n",
    "\n",
    "REQUISITOS PREVIOS:\n",
    "- Ejecutar PASO_09_Modelado_ML_Completo.py\n",
    "- Tener datasets en mismo directorio\n",
    "- Instalar: pip install shap --break-system-packages\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# SECCIÃ“N 0: IMPORTS Y CONFIGURACIÃ“N\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PASO 10: MEJORA Y VALIDACIÃ“N AVANZADA DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LibrerÃ­as estÃ¡ndar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    StratifiedKFold,\n",
    "    RepeatedStratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    classification_report, make_scorer\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# SHAP para explicabilidad\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(\"âœ“ SHAP disponible\")\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"âš ï¸ SHAP no disponible. Instalar: pip install shap --break-system-packages\")\n",
    "    print(\"   Continuando sin SHAP...\")\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciÃ³n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Semilla global\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"âœ“ Imports completados\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# SECCIÃ“N 1: CARGAR DATOS Y RECREAR MODELOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SECCIÃ“N 1: CARGA DE DATOS Y PREPARACIÃ“N\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# AJUSTAR RUTAS\n",
    "PATH_DATASET_A = '../documentos_generados/PCOS_data_transformado.csv'\n",
    "PATH_DATASET_B = 'PCOS_data_FINAL_sin_multicolinealidad.csv'\n",
    "TARGET_COL = 'SOP (S/N)'\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "print(\"Cargando datasets...\")\n",
    "df_trees = pd.read_csv(PATH_DATASET_A)\n",
    "df_logit = pd.read_csv(PATH_DATASET_B)\n",
    "\n",
    "# LIMPIAR VALORES CON FORMATO '[5E-1]' o similares\n",
    "print(\"Limpiando formato de datos...\")\n",
    "\n",
    "def limpiar_columna(col):\n",
    "    \"\"\"Limpia valores con formato [5E-1] convirtiÃ©ndolos a float\"\"\"\n",
    "    if col.dtype == 'object':\n",
    "        # Remover corchetes y convertir\n",
    "        col = col.astype(str).str.replace('[', '', regex=False).str.replace(']', '', regex=False)\n",
    "        try:\n",
    "            col = pd.to_numeric(col, errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "    return col\n",
    "\n",
    "# Aplicar limpieza a todas las columnas numÃ©ricas\n",
    "for col in df_trees.columns:\n",
    "    if col != TARGET_COL:\n",
    "        df_trees[col] = limpiar_columna(df_trees[col])\n",
    "\n",
    "for col in df_logit.columns:\n",
    "    if col != TARGET_COL:\n",
    "        df_logit[col] = limpiar_columna(df_logit[col])\n",
    "\n",
    "# Verificar si quedaron NaN despuÃ©s de limpieza\n",
    "nans_trees = df_trees.isnull().sum().sum()\n",
    "nans_logit = df_logit.isnull().sum().sum()\n",
    "\n",
    "if nans_trees > 0 or nans_logit > 0:\n",
    "    print(f\"âš ï¸ NaN detectados despuÃ©s de limpieza:\")\n",
    "    print(f\"   Dataset A: {nans_trees} NaN\")\n",
    "    print(f\"   Dataset B: {nans_logit} NaN\")\n",
    "    print(\"   Rellenando con mediana por columna...\")\n",
    "    \n",
    "    # Rellenar con mediana\n",
    "    for col in df_trees.select_dtypes(include=[np.number]).columns:\n",
    "        if df_trees[col].isnull().any():\n",
    "            df_trees[col].fillna(df_trees[col].median(), inplace=True)\n",
    "    \n",
    "    for col in df_logit.select_dtypes(include=[np.number]).columns:\n",
    "        if df_logit[col].isnull().any():\n",
    "            df_logit[col].fillna(df_logit[col].median(), inplace=True)\n",
    "\n",
    "# Verificar tipos de datos\n",
    "print(\"Verificando tipos de datos...\")\n",
    "non_numeric_trees = df_trees.drop(TARGET_COL, axis=1).select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "non_numeric_logit = df_logit.drop(TARGET_COL, axis=1).select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "if non_numeric_trees:\n",
    "    print(f\"âš ï¸ Columnas no numÃ©ricas en Dataset A: {non_numeric_trees}\")\n",
    "    print(\"   Convirtiendo a numÃ©rico...\")\n",
    "    for col in non_numeric_trees:\n",
    "        df_trees[col] = pd.to_numeric(df_trees[col], errors='coerce')\n",
    "        if df_trees[col].isnull().any():\n",
    "            df_trees[col].fillna(df_trees[col].median(), inplace=True)\n",
    "\n",
    "if non_numeric_logit:\n",
    "    print(f\"âš ï¸ Columnas no numÃ©ricas en Dataset B: {non_numeric_logit}\")\n",
    "    print(\"   Convirtiendo a numÃ©rico...\")\n",
    "    for col in non_numeric_logit:\n",
    "        df_logit[col] = pd.to_numeric(df_logit[col], errors='coerce')\n",
    "        if df_logit[col].isnull().any():\n",
    "            df_logit[col].fillna(df_logit[col].median(), inplace=True)\n",
    "\n",
    "print(f\"âœ“ Dataset A limpio: {df_trees.shape}\")\n",
    "print(f\"âœ“ Dataset B limpio: {df_logit.shape}\")\n",
    "print()\n",
    "\n",
    "# Train/Test Split\n",
    "print(\"Preparando datos...\")\n",
    "\n",
    "# Dataset A (Ã¡rboles)\n",
    "X_trees = df_trees.drop(TARGET_COL, axis=1)\n",
    "y_trees = df_trees[TARGET_COL]\n",
    "X_train_trees, X_test_trees, y_train_trees, y_test_trees = train_test_split(\n",
    "    X_trees, y_trees, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_trees\n",
    ")\n",
    "\n",
    "# Dataset B (logÃ­stica)\n",
    "X_logit = df_logit.drop(TARGET_COL, axis=1)\n",
    "y_logit = df_logit[TARGET_COL]\n",
    "X_train_logit, X_test_logit, y_train_logit, y_test_logit = train_test_split(\n",
    "    X_logit, y_logit, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_logit\n",
    ")\n",
    "\n",
    "# SMOTE\n",
    "smote_trees = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_trees_balanced, y_train_trees_balanced = smote_trees.fit_resample(\n",
    "    X_train_trees, y_train_trees\n",
    ")\n",
    "\n",
    "smote_logit = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_logit_balanced, y_train_logit_balanced = smote_logit.fit_resample(\n",
    "    X_train_logit, y_train_logit\n",
    ")\n",
    "\n",
    "# Escalamiento\n",
    "scaler_logit = StandardScaler()\n",
    "X_train_logit_scaled = pd.DataFrame(\n",
    "    scaler_logit.fit_transform(X_train_logit_balanced),\n",
    "    columns=X_train_logit.columns\n",
    ")\n",
    "X_test_logit_scaled = pd.DataFrame(\n",
    "    scaler_logit.transform(X_test_logit),\n",
    "    columns=X_test_logit.columns\n",
    ")\n",
    "\n",
    "scaler_knn = StandardScaler()\n",
    "X_train_trees_scaled = pd.DataFrame(\n",
    "    scaler_knn.fit_transform(X_train_trees_balanced),\n",
    "    columns=X_train_trees.columns\n",
    ")\n",
    "X_test_trees_scaled = pd.DataFrame(\n",
    "    scaler_knn.transform(X_test_trees),\n",
    "    columns=X_test_trees.columns\n",
    ")\n",
    "\n",
    "print(\"âœ“ Datos preparados (split, SMOTE, escalamiento)\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 1: MÃ‰TRICAS CLÃNICAS EXTENDIDAS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 1: MÃ‰TRICAS CLÃNICAS EXTENDIDAS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "def calcular_metricas_clinicas(y_true, y_pred, model_name):\n",
    "    \"\"\"Calcula mÃ©tricas clÃ­nicas detalladas\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # MÃ©tricas bÃ¡sicas\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # MÃ©tricas clÃ­nicas\n",
    "    sensibilidad = tp / (tp + fn) if (tp + fn) > 0 else 0  # = Recall\n",
    "    especificidad = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0  # Positive Predictive Value = Precision\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "    \n",
    "    # Likelihood Ratios\n",
    "    lr_pos = sensibilidad / (1 - especificidad) if especificidad < 1 else np.inf\n",
    "    lr_neg = (1 - sensibilidad) / especificidad if especificidad > 0 else np.inf\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'TN': int(tn), 'FP': int(fp), 'FN': int(fn), 'TP': int(tp),\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (PPV)': ppv,\n",
    "        'Recall (Sensibilidad)': sensibilidad,\n",
    "        'Especificidad': especificidad,\n",
    "        'F1-Score': f1,\n",
    "        'NPV': npv,\n",
    "        'LR+': lr_pos,\n",
    "        'LR-': lr_neg\n",
    "    }\n",
    "\n",
    "# Entrenar modelos rÃ¡pidos (sin Grid Search por velocidad)\n",
    "print(\"Entrenando modelos para evaluaciÃ³n...\")\n",
    "print()\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr_model = LogisticRegression(C=0.1, max_iter=1000, random_state=RANDOM_STATE)\n",
    "lr_model.fit(X_train_logit_scaled, y_train_logit_balanced)\n",
    "y_pred_lr = lr_model.predict(X_test_logit_scaled)\n",
    "\n",
    "# 2. Random Forest (con 40 features - del Paso 9)\n",
    "# Recrear RFE para obtener las 40 features\n",
    "rf_estimator = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "rfe = RFE(estimator=rf_estimator, n_features_to_select=40, step=1)\n",
    "rfe.fit(X_train_trees_balanced, y_train_trees_balanced)\n",
    "X_train_rf = rfe.transform(X_train_trees_balanced)\n",
    "X_test_rf = rfe.transform(X_test_trees)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=10, min_samples_split=5, \n",
    "    min_samples_leaf=1, random_state=RANDOM_STATE\n",
    ")\n",
    "rf_model.fit(X_train_rf, y_train_trees_balanced)\n",
    "y_pred_rf = rf_model.predict(X_test_rf)\n",
    "\n",
    "# 3. XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "    subsample=0.8, random_state=RANDOM_STATE, eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train_trees_balanced, y_train_trees_balanced)\n",
    "y_pred_xgb = xgb_model.predict(X_test_trees)\n",
    "\n",
    "# 4. KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=7, weights='distance', metric='manhattan')\n",
    "knn_model.fit(X_train_trees_scaled, y_train_trees_balanced)\n",
    "y_pred_knn = knn_model.predict(X_test_trees_scaled)\n",
    "\n",
    "print(\"âœ“ Modelos entrenados\")\n",
    "print()\n",
    "\n",
    "# Calcular mÃ©tricas clÃ­nicas\n",
    "print(\"MÃ‰TRICAS CLÃNICAS EXTENDIDAS:\")\n",
    "print()\n",
    "\n",
    "metricas_clinicas = []\n",
    "metricas_clinicas.append(calcular_metricas_clinicas(y_test_logit, y_pred_lr, 'Logistic Regression'))\n",
    "metricas_clinicas.append(calcular_metricas_clinicas(y_test_trees, y_pred_rf, 'Random Forest'))\n",
    "metricas_clinicas.append(calcular_metricas_clinicas(y_test_trees, y_pred_xgb, 'XGBoost'))\n",
    "metricas_clinicas.append(calcular_metricas_clinicas(y_test_trees, y_pred_knn, 'KNN'))\n",
    "\n",
    "df_clinicas = pd.DataFrame(metricas_clinicas)\n",
    "\n",
    "print(df_clinicas.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Guardar\n",
    "df_clinicas.to_csv('metricas_clinicas_extendidas.csv', index=False)\n",
    "print(\"âœ“ Guardado: metricas_clinicas_extendidas.csv\")\n",
    "print()\n",
    "\n",
    "# InterpretaciÃ³n clÃ­nica\n",
    "print(\"INTERPRETACIÃ“N CLÃNICA:\")\n",
    "print()\n",
    "print(\"Sensibilidad (Recall): % de SOP reales detectados\")\n",
    "print(\"  â†’ Alto = Menos falsos negativos (no miss casos SOP)\")\n",
    "print()\n",
    "print(\"Especificidad: % de No-SOP correctamente identificados\")\n",
    "print(\"  â†’ Alto = Menos falsos positivos (menos alarmas falsas)\")\n",
    "print()\n",
    "print(\"PPV (Precision): Si predice SOP, Â¿cuÃ¡l es probabilidad real?\")\n",
    "print(\"  â†’ Alto = PredicciÃ³n SOP confiable\")\n",
    "print()\n",
    "print(\"NPV: Si predice No-SOP, Â¿cuÃ¡l es probabilidad real?\")\n",
    "print(\"  â†’ Alto = PredicciÃ³n negativa confiable\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 2: EXPLICABILIDAD CON SHAP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 2: EXPLICABILIDAD CON SHAP VALUES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "if not SHAP_AVAILABLE:\n",
    "    print(\"âš ï¸ SHAP no disponible. Saltando esta secciÃ³n.\")\n",
    "    print(\"   Para instalar: pip install shap --break-system-packages\")\n",
    "    print()\n",
    "else:\n",
    "    try:\n",
    "        print(\"Calculando SHAP values para XGBoost...\")\n",
    "        print(\"(Esto puede tardar 2-3 minutos)\")\n",
    "        print()\n",
    "        \n",
    "        # SHAP para XGBoost (mÃ¡s rÃ¡pido)\n",
    "        explainer_xgb = shap.TreeExplainer(xgb_model)\n",
    "        shap_values_xgb = explainer_xgb.shap_values(X_test_trees)\n",
    "        \n",
    "        # Summary plot XGBoost\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_xgb, X_test_trees, plot_type=\"bar\", show=False, max_display=15)\n",
    "        plt.title('XGBoost - Top 15 Features (SHAP)', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_xgboost_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"âœ“ Guardado: shap_xgboost_summary.png\")\n",
    "        \n",
    "        # Beeswarm plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_xgb, X_test_trees, show=False, max_display=15)\n",
    "        plt.title('XGBoost - DistribuciÃ³n de Impacto (SHAP)', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_xgboost_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"âœ“ Guardado: shap_xgboost_beeswarm.png\")\n",
    "        \n",
    "        # Extraer valores promedio\n",
    "        shap_importance = pd.DataFrame({\n",
    "            'Feature': X_test_trees.columns,\n",
    "            'SHAP_Mean_Abs': np.abs(shap_values_xgb).mean(axis=0)\n",
    "        }).sort_values('SHAP_Mean_Abs', ascending=False)\n",
    "        \n",
    "        shap_importance.to_csv('shap_importance_xgboost.csv', index=False)\n",
    "        print(\"âœ“ Guardado: shap_importance_xgboost.csv\")\n",
    "        print()\n",
    "        \n",
    "        print(\"TOP 10 FEATURES (SHAP):\")\n",
    "        print(shap_importance.head(10).to_string(index=False))\n",
    "        print()\n",
    "        \n",
    "        # SHAP para Random Forest (opcional, mÃ¡s lento)\n",
    "        print(\"Calculando SHAP values para Random Forest...\")\n",
    "        print(\"(Esto puede tardar 3-5 minutos)\")\n",
    "        print()\n",
    "        \n",
    "        explainer_rf = shap.TreeExplainer(rf_model)\n",
    "        shap_values_rf = explainer_rf.shap_values(X_test_rf)\n",
    "        \n",
    "        # Si es clasificaciÃ³n binaria, tomar clase positiva\n",
    "        if isinstance(shap_values_rf, list):\n",
    "            shap_values_rf = shap_values_rf[1]\n",
    "        \n",
    "        # Summary plot RF\n",
    "        selected_features = X_train_trees_balanced.columns[rfe.support_].tolist()\n",
    "        X_test_rf_df = pd.DataFrame(X_test_rf, columns=selected_features)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values_rf, X_test_rf_df, plot_type=\"bar\", show=False, max_display=15)\n",
    "        plt.title('Random Forest - Top 15 Features (SHAP)', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('shap_rf_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(\"âœ“ Guardado: shap_rf_summary.png\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error en SHAP: {str(e)}\")\n",
    "        print(\"   Continuando sin anÃ¡lisis SHAP...\")\n",
    "        print()\n",
    "        SHAP_AVAILABLE = False\n",
    "\n",
    "print(\"âœ“ FASE 2 completada\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 3: ENSEMBLE (VOTING CLASSIFIER)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 3: ENSEMBLE - VOTING CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"Creando ensemble RF + XGBoost...\")\n",
    "print()\n",
    "\n",
    "# Ensemble con los mejores modelos\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_model),\n",
    "        ('xgb', xgb_model)\n",
    "    ],\n",
    "    voting='soft',  # Usa probabilidades\n",
    "    weights=[1, 1]  # Mismo peso\n",
    ")\n",
    "\n",
    "# Entrenar ensemble (en los datos transformados de RF)\n",
    "ensemble.fit(X_train_rf, y_train_trees_balanced)\n",
    "y_pred_ensemble = ensemble.predict(X_test_rf)\n",
    "y_pred_proba_ensemble = ensemble.predict_proba(X_test_rf)[:, 1]\n",
    "\n",
    "# MÃ©tricas ensemble\n",
    "accuracy_ensemble = accuracy_score(y_test_trees, y_pred_ensemble)\n",
    "precision_ensemble = precision_score(y_test_trees, y_pred_ensemble)\n",
    "recall_ensemble = recall_score(y_test_trees, y_pred_ensemble)\n",
    "f1_ensemble = f1_score(y_test_trees, y_pred_ensemble)\n",
    "auc_ensemble = roc_auc_score(y_test_trees, y_pred_proba_ensemble)\n",
    "\n",
    "print(\"RESULTADOS ENSEMBLE:\")\n",
    "print(f\"  Accuracy:  {accuracy_ensemble:.4f}\")\n",
    "print(f\"  Precision: {precision_ensemble:.4f}\")\n",
    "print(f\"  Recall:    {recall_ensemble:.4f}\")\n",
    "print(f\"  F1-Score:  {f1_ensemble:.4f}\")\n",
    "print(f\"  ROC-AUC:   {auc_ensemble:.4f}\")\n",
    "print()\n",
    "\n",
    "# ComparaciÃ³n con modelos individuales\n",
    "print(\"COMPARACIÃ“N:\")\n",
    "print()\n",
    "comparison_ensemble = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'XGBoost', 'Ensemble (RF+XGB)'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test_trees, y_pred_rf),\n",
    "        accuracy_score(y_test_trees, y_pred_xgb),\n",
    "        accuracy_ensemble\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test_trees, y_pred_rf),\n",
    "        precision_score(y_test_trees, y_pred_xgb),\n",
    "        precision_ensemble\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test_trees, y_pred_rf),\n",
    "        recall_score(y_test_trees, y_pred_xgb),\n",
    "        recall_ensemble\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test_trees, y_pred_rf),\n",
    "        f1_score(y_test_trees, y_pred_xgb),\n",
    "        f1_ensemble\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_test_trees, rf_model.predict_proba(X_test_rf)[:, 1]),\n",
    "        roc_auc_score(y_test_trees, xgb_model.predict_proba(X_test_trees)[:, 1]),\n",
    "        auc_ensemble\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_ensemble.to_string(index=False))\n",
    "print()\n",
    "\n",
    "comparison_ensemble.to_csv('ensemble_comparison.csv', index=False)\n",
    "print(\"âœ“ Guardado: ensemble_comparison.csv\")\n",
    "print()\n",
    "\n",
    "# MÃ©tricas clÃ­nicas del ensemble\n",
    "metricas_ensemble = calcular_metricas_clinicas(y_test_trees, y_pred_ensemble, 'Ensemble (RF+XGB)')\n",
    "print(\"MÃ‰TRICAS CLÃNICAS ENSEMBLE:\")\n",
    "print(pd.DataFrame([metricas_ensemble]).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 4: OPTIMIZACIÃ“N RFE (REDUCIR A 20 FEATURES)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 4: OPTIMIZACIÃ“N RFE - REDUCIR A 20 FEATURES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"Probando RFE con diferentes nÃºmeros de features...\")\n",
    "print()\n",
    "\n",
    "n_features_range = [10, 15, 20, 25, 30]\n",
    "rfe_optimization_results = []\n",
    "\n",
    "for n_features in n_features_range:\n",
    "    rf_estimator_opt = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, min_samples_split=5,\n",
    "        random_state=RANDOM_STATE, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    rfe_opt = RFE(estimator=rf_estimator_opt, n_features_to_select=n_features, step=1)\n",
    "    rfe_opt.fit(X_train_trees_balanced, y_train_trees_balanced)\n",
    "    \n",
    "    X_train_rfe_opt = rfe_opt.transform(X_train_trees_balanced)\n",
    "    X_test_rfe_opt = rfe_opt.transform(X_test_trees)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        rf_estimator_opt, X_train_rfe_opt, y_train_trees_balanced,\n",
    "        cv=5, scoring='f1'\n",
    "    )\n",
    "    \n",
    "    # Test performance\n",
    "    rf_estimator_opt.fit(X_train_rfe_opt, y_train_trees_balanced)\n",
    "    y_pred_opt = rf_estimator_opt.predict(X_test_rfe_opt)\n",
    "    \n",
    "    test_f1 = f1_score(y_test_trees, y_pred_opt)\n",
    "    test_acc = accuracy_score(y_test_trees, y_pred_opt)\n",
    "    \n",
    "    rfe_optimization_results.append({\n",
    "        'n_features': n_features,\n",
    "        'cv_f1_mean': cv_scores.mean(),\n",
    "        'cv_f1_std': cv_scores.std(),\n",
    "        'test_f1': test_f1,\n",
    "        'test_accuracy': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"  {n_features} features: CV F1={cv_scores.mean():.4f}Â±{cv_scores.std():.4f}, Test F1={test_f1:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "df_rfe_opt = pd.DataFrame(rfe_optimization_results)\n",
    "df_rfe_opt.to_csv('rfe_optimization_results.csv', index=False)\n",
    "print(\"âœ“ Guardado: rfe_optimization_results.csv\")\n",
    "print()\n",
    "\n",
    "# Mejor configuraciÃ³n\n",
    "best_n = df_rfe_opt.loc[df_rfe_opt['test_f1'].idxmax(), 'n_features']\n",
    "print(f\"âœ“ Mejor nÃºmero de features: {int(best_n)}\")\n",
    "print()\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_rfe_opt['n_features'], df_rfe_opt['test_f1'], 'o-', linewidth=2, markersize=8, label='Test F1')\n",
    "plt.plot(df_rfe_opt['n_features'], df_rfe_opt['cv_f1_mean'], 's--', linewidth=2, markersize=8, label='CV F1 Mean')\n",
    "plt.xlabel('NÃºmero de Features', fontsize=12)\n",
    "plt.ylabel('F1-Score', fontsize=12)\n",
    "plt.title('OptimizaciÃ³n RFE - Performance vs NÃºmero de Features', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rfe_optimization_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ“ Guardado: rfe_optimization_curve.png\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# FASE 5: VALIDACIÃ“N REPETIDA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FASE 5: VALIDACIÃ“N REPETIDA (REPEATED STRATIFIED K-FOLD)\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"ConfiguraciÃ³n: 5-Fold Ã— 3 repeticiones = 15 evaluaciones\")\n",
    "print(\"(Esto puede tardar 5-10 minutos)\")\n",
    "print()\n",
    "\n",
    "# Configurar validaciÃ³n repetida\n",
    "repeated_cv = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Scoring mÃºltiple\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "modelos_validacion = {\n",
    "    'Logistic Regression': (lr_model, X_train_logit_scaled, y_train_logit_balanced),\n",
    "    'Random Forest (40f)': (rf_model, X_train_rf, y_train_trees_balanced),\n",
    "    'XGBoost': (xgb_model, X_train_trees_balanced, y_train_trees_balanced),\n",
    "    'Ensemble (RF+XGB)': (ensemble, X_train_rf, y_train_trees_balanced)\n",
    "}\n",
    "\n",
    "resultados_validacion = []\n",
    "\n",
    "for model_name, (model, X_train, y_train) in modelos_validacion.items():\n",
    "    print(f\"Validando {model_name}...\")\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        cv=repeated_cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    resultados_validacion.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy_mean': cv_results['test_accuracy'].mean(),\n",
    "        'Accuracy_std': cv_results['test_accuracy'].std(),\n",
    "        'Precision_mean': cv_results['test_precision'].mean(),\n",
    "        'Precision_std': cv_results['test_precision'].std(),\n",
    "        'Recall_mean': cv_results['test_recall'].mean(),\n",
    "        'Recall_std': cv_results['test_recall'].std(),\n",
    "        'F1_mean': cv_results['test_f1'].mean(),\n",
    "        'F1_std': cv_results['test_f1'].std(),\n",
    "        'ROC_AUC_mean': cv_results['test_roc_auc'].mean(),\n",
    "        'ROC_AUC_std': cv_results['test_roc_auc'].std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  âœ“ F1 = {cv_results['test_f1'].mean():.4f} Â± {cv_results['test_f1'].std():.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "df_validacion = pd.DataFrame(resultados_validacion)\n",
    "\n",
    "print(\"RESULTADOS VALIDACIÃ“N REPETIDA:\")\n",
    "print()\n",
    "print(df_validacion.to_string(index=False))\n",
    "print()\n",
    "\n",
    "df_validacion.to_csv('validacion_repetida_results.csv', index=False)\n",
    "print(\"âœ“ Guardado: validacion_repetida_results.csv\")\n",
    "print()\n",
    "\n",
    "# VisualizaciÃ³n de intervalos de confianza\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('ValidaciÃ³n Repetida - Intervalos de Confianza (95%)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metricas_plot = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "for idx, metric in enumerate(metricas_plot):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    means = df_validacion[f'{metric}_mean']\n",
    "    stds = df_validacion[f'{metric}_std']\n",
    "    models = df_validacion['Model']\n",
    "    \n",
    "    # Intervalos 95% (aprox 2*std)\n",
    "    ci = 1.96 * stds\n",
    "    \n",
    "    ax.errorbar(range(len(models)), means, yerr=ci, fmt='o', markersize=8, \n",
    "                capsize=5, capthick=2, linewidth=2)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_title(f'{metric} (Mean Â± 95% CI)', fontsize=12, fontweight='bold')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    ax.set_ylim([0.7, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('validacion_repetida_intervals.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ“ Guardado: validacion_repetida_intervals.png\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# SECCIÃ“N FINAL: REPORTE Y RECOMENDACIONES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPORTE FINAL Y RECOMENDACIONES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Determinar mejor modelo\n",
    "best_model_idx = df_validacion['F1_mean'].idxmax()\n",
    "best_model_name = df_validacion.loc[best_model_idx, 'Model']\n",
    "best_f1_mean = df_validacion.loc[best_model_idx, 'F1_mean']\n",
    "best_f1_std = df_validacion.loc[best_model_idx, 'F1_std']\n",
    "\n",
    "print(\"ðŸ† MODELO RECOMENDADO:\")\n",
    "print(f\"   {best_model_name}\")\n",
    "print(f\"   F1-Score: {best_f1_mean:.4f} Â± {best_f1_std:.4f}\")\n",
    "print()\n",
    "\n",
    "# ComparaciÃ³n ensemble vs individuales\n",
    "if 'Ensemble' in best_model_name:\n",
    "    print(\"âœ… ENSEMBLE SUPERIOR: Combinar RF + XGBoost mejora performance\")\n",
    "else:\n",
    "    mejora_ensemble = df_validacion[df_validacion['Model'] == 'Ensemble (RF+XGB)']['F1_mean'].values[0]\n",
    "    mejora_individual = df_validacion[df_validacion['Model'] == best_model_name]['F1_mean'].values[0]\n",
    "    \n",
    "    if mejora_ensemble > mejora_individual:\n",
    "        print(f\"âš ï¸ ENSEMBLE LIGERAMENTE MEJOR: {mejora_ensemble:.4f} vs {mejora_individual:.4f}\")\n",
    "        print(\"   Considerar usar ensemble para producciÃ³n\")\n",
    "    else:\n",
    "        print(f\"âœ… MODELO INDIVIDUAL Ã“PTIMO: {best_model_name}\")\n",
    "        print(\"   Ensemble no aporta mejora significativa\")\n",
    "\n",
    "print()\n",
    "\n",
    "# AnÃ¡lisis RFE\n",
    "best_n_features = df_rfe_opt.loc[df_rfe_opt['test_f1'].idxmax(), 'n_features']\n",
    "f1_40_features = df_rfe_opt[df_rfe_opt['n_features'] == 30]['test_f1'].values\n",
    "f1_best_n = df_rfe_opt.loc[df_rfe_opt['test_f1'].idxmax(), 'test_f1']\n",
    "\n",
    "if len(f1_40_features) > 0 and (f1_best_n - f1_40_features[0]) < 0.01:\n",
    "    print(f\"ðŸ“Š REDUCCIÃ“N DE FEATURES EXITOSA:\")\n",
    "    print(f\"   {int(best_n_features)} features mantienen performance\")\n",
    "    print(f\"   Simplificar modelo sin perder accuracy\")\n",
    "else:\n",
    "    print(\"âš ï¸ MANTENER 40 FEATURES:\")\n",
    "    print(\"   ReducciÃ³n afecta significativamente performance\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Recomendaciones finales\n",
    "print(\"ðŸ“‹ RECOMENDACIONES PARA TESIS/PAPER:\")\n",
    "print()\n",
    "print(\"1. MODELO FINAL:\")\n",
    "print(f\"   â†’ Usar {best_model_name}\")\n",
    "print(f\"   â†’ F1 = {best_f1_mean:.4f} (validaciÃ³n cruzada repetida)\")\n",
    "print()\n",
    "\n",
    "print(\"2. MÃ‰TRICAS CLÃNICAS A REPORTAR:\")\n",
    "# Obtener mÃ©tricas clÃ­nicas del mejor modelo\n",
    "if 'Ensemble' in best_model_name:\n",
    "    best_metrics = metricas_ensemble\n",
    "else:\n",
    "    best_metrics = [m for m in metricas_clinicas if m['Model'] == best_model_name.split(' (')[0]][0]\n",
    "\n",
    "print(f\"   â†’ Sensibilidad: {best_metrics['Recall (Sensibilidad)']:.1%}\")\n",
    "print(f\"   â†’ Especificidad: {best_metrics['Especificidad']:.1%}\")\n",
    "print(f\"   â†’ PPV: {best_metrics['Precision (PPV)']:.1%}\")\n",
    "print(f\"   â†’ NPV: {best_metrics['NPV']:.1%}\")\n",
    "print()\n",
    "\n",
    "print(\"3. EXPLICABILIDAD:\")\n",
    "if SHAP_AVAILABLE:\n",
    "    print(\"   â†’ Usar SHAP beeswarm plot para paper\")\n",
    "    print(\"   â†’ Discutir top 5 variables clÃ­nicamente\")\n",
    "else:\n",
    "    print(\"   â†’ Instalar SHAP para anÃ¡lisis de explicabilidad\")\n",
    "print()\n",
    "\n",
    "print(\"4. VALIDACIÃ“N:\")\n",
    "print(\"   â†’ Mencionar validaciÃ³n repetida 5Ã—3\")\n",
    "print(\"   â†’ Reportar intervalos de confianza\")\n",
    "print(\"   â†’ Robustez demostrada\")\n",
    "print()\n",
    "\n",
    "# Crear reporte JSON\n",
    "reporte_final = {\n",
    "    'fecha': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'mejor_modelo': best_model_name,\n",
    "    'f1_score': {\n",
    "        'mean': float(best_f1_mean),\n",
    "        'std': float(best_f1_std),\n",
    "        'ci_95': [float(best_f1_mean - 1.96*best_f1_std), \n",
    "                  float(best_f1_mean + 1.96*best_f1_std)]\n",
    "    },\n",
    "    'metricas_clinicas': {\n",
    "        'sensibilidad': float(best_metrics['Recall (Sensibilidad)']),\n",
    "        'especificidad': float(best_metrics['Especificidad']),\n",
    "        'ppv': float(best_metrics['Precision (PPV)']),\n",
    "        'npv': float(best_metrics['NPV'])\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'TP': int(best_metrics['TP']),\n",
    "        'FP': int(best_metrics['FP']),\n",
    "        'TN': int(best_metrics['TN']),\n",
    "        'FN': int(best_metrics['FN'])\n",
    "    },\n",
    "    'rfe_optimizacion': {\n",
    "        'mejor_n_features': int(best_n_features),\n",
    "        'features_originales': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('reporte_final_paso10.json', 'w') as f:\n",
    "    json.dump(reporte_final, f, indent=2)\n",
    "\n",
    "print(\"âœ“ Reporte guardado: reporte_final_paso10.json\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# ARCHIVOS GENERADOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ARCHIVOS GENERADOS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"ðŸ“ MÃ‰TRICAS Y RESULTADOS:\")\n",
    "print(\"  1. metricas_clinicas_extendidas.csv\")\n",
    "print(\"  2. ensemble_comparison.csv\")\n",
    "print(\"  3. rfe_optimization_results.csv\")\n",
    "print(\"  4. validacion_repetida_results.csv\")\n",
    "if SHAP_AVAILABLE:\n",
    "    print(\"  5. shap_importance_xgboost.csv\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ“Š VISUALIZACIONES:\")\n",
    "if SHAP_AVAILABLE:\n",
    "    print(\"  6. shap_xgboost_summary.png\")\n",
    "    print(\"  7. shap_xgboost_beeswarm.png\")\n",
    "    print(\"  8. shap_rf_summary.png\")\n",
    "print(\"  9. rfe_optimization_curve.png\")\n",
    "print(\"  10. validacion_repetida_intervals.png\")\n",
    "print()\n",
    "\n",
    "print(\"ðŸ“„ REPORTES:\")\n",
    "print(\"  11. reporte_final_paso10.json\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âœ… PASO 10 COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"ðŸŽ‰ ANÃLISIS COMPLETO\")\n",
    "print()\n",
    "print(\"Has completado:\")\n",
    "print(\"  âœ“ MÃ©tricas clÃ­nicas extendidas\")\n",
    "print(\"  âœ“ Explicabilidad (SHAP)\")\n",
    "print(\"  âœ“ Ensemble optimization\")\n",
    "print(\"  âœ“ RFE refinement\")\n",
    "print(\"  âœ“ ValidaciÃ³n cruzada repetida\")\n",
    "print()\n",
    "print(\"Tu proyecto estÃ¡ listo para:\")\n",
    "print(\"  â†’ Defensa de tesis\")\n",
    "print(\"  â†’ PublicaciÃ³n cientÃ­fica\")\n",
    "print(\"  â†’ PresentaciÃ³n a comitÃ© biomÃ©dico\")\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
